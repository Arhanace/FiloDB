# Required: Load by setting -Dfilodb.kafka.clients.config=path/to/kafka-client.properties

# Kafka `bootstrap.servers` is common for any producer and consumer.
# These should be passed in via config as: `filodb.kafka.bootstrap.servers`
# to be fed into any producer, consumer or streams config.
bootstrap.servers="localhost:9092"

# Producer-specific defaults to override.
# Settings should be prefixed with `producer`
# partitioner = "filodb.kafka.temp.ShardPartitioner"
# User will need to pass in the correct configurations for their type
producer.value.serializer=org.example.CustomSerializer
# ack=1 from partition leader only, test:"all" slowest, safest
producer.acks=1

# Consumer-specific defaults to override.
# Settings should be prefixed with `consumer`
consumer.value.deserializer=org.example.CustomDeserializer
consumer.partition.assignment.strategy=org.apache.kafka.clients.consumer.RangeAssignor

# Set to override kafka default: auto.offset.reset=earliest
# consumer.auto.offset.reset=

# Set to override kafka default: consumer.enable.auto.commit=false
# Note: To guarantee at-least-once processing semantics, Kafka Streams
# will always override this consumer config value to false in order to
# turn off auto committing. Instead, consumers will only commit explicitly
# via commitSync calls when Kafka Streams library or users decide to commit
# the current processing state.
# consumer.enable.auto.commit

# Streams-specific defaults to override.
# Settings should be prefixed with `streams`
# Currently Kafka Streams applications can only talk to a single Kafka cluster specified by this config value.
# In the future Kafka Streams will be able to support connecting to different Kafka clusters for reading input
# streams and/or writing output streams.

# User will need to pass in the correct configurations for their type
streams.value.serde=org.example.CustomSerde

# Each stream processing application must have a unique id.
# The same id must be given to all instances of the application.
application.id=filodb.streams

# The amount of time in milliseconds to block waiting for input.
# Kafka defaults to 100
streams.poll.ms=100
# Kafka defaults to 500, but this has issues. Starting low, TODO tune.
streams.max.poll.records=250
streams.max.poll.interval.ms=5000
# The frequency with which to save the position of the processor.
# Kafka defaults to 30000
streams.commit.interval.ms=30000
# The size of the TCP receive buffer (SO_RCVBUF) to use when reading data.
# If the value is -1, the OS default will be used. Defaults to 32768.
streams.receive.buffer.bytes=32768
# The maximum number of records to buffer per partition. Kafka defaults to 1000.
streams.buffered.records.per.partition=1000
streams.session.timeout=20000
