####################################
# Sample FiloDB Kafka Source Config File #
####################################
# Ingestion source metadata
dataset = "example"
columns = ["a", "b", "c"]
num-shards = 128
min-num-nodes = 5
sourcefactory = "filodb.kafka.KafkaIngestionStreamFactory"

# Supports all the features of Typesafe config, such as including other .conf files
# or even Kafka properties files.
# Any included Kafka properties minus the filo- configs are passed through as is.
sourceconfig {
  ##
  # Required FiloDB Kafka properties
  # 1. Your Kafka topic name
  # 2. Your record converter: converts deserialized message to Seq[IngestRecord]
  filo-topic-name = "your.event.stream"
  filo-record-converter = "filodb.kafka.StringRecordConverter"
  # end FiloDB Kafka properties
  ##

  ##
  # Kafka properties
  # The shard number is supposed to be put into the Kafka message key at producer side
  value.deserializer = "org.example.CustomDeserializer"

  # Kafka brokers address and port. Accepts both
  # HOCON/Typesafe Config style lists, as below, or a comma-separated string.
  bootstrap.servers = [
    "localhost:9092"
  ]
  # end Kafka properties
  ##

  ##
  # Custom properties
  # You can also pass in any custom key/value pairs to your kafka client
  my.custom.client.namespace = "custom.value"

  # Values controlling in-memory store chunking, flushing, etc.
  store {
    # Interval it takes to flush ALL time series in a shard.  This time is further divided by groups-per-shard
    flush-interval = 2 minutes

    # TTL for on-disk / C* data.  Data older than this may be purged.
    disk-time-to-live = 3 days

    # amount of time paged chunks should be retained in memory
    demand-paged-chunk-retention-period = 72 hours

    max-chunks-size = 500

    # Fixed amount of memory, in MBs, to allocate for encoded chunks per shard
    shard-memory-mb = 512

    # Max # of partitions or time series the WriteBufferPool can allocate and that can be ingested at a time
    max-num-partitions = 100000

    # Number of subgroups within each shard.  Persistence to a ChunkSink occurs one subgroup at a time, as does
    # recovery from failure.  This many batches of flushes must occur to cover persistence of every partition
    groups-per-shard = 60
  }

}
