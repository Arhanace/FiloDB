    dataset = "timeseries"
    num-shards = 4
    min-num-nodes = 2
    # Length of chunks to be written, roughly
    sourcefactory = "filodb.kafka.KafkaIngestionStreamFactory"
    sourceconfig {
      group.id = "filo-db-timeseries-ingestion"
      filo-topic-name = "timeseries-dev"
      bootstrap.servers = "localhost:9092"
      filo-record-converter = "filodb.timeseries.TimeseriesSampleConverter"
      value.deserializer= "org.apache.kafka.common.serialization.StringDeserializer"

      # Values controlling in-memory store chunking, flushing, etc.
      store {
        # Interval it takes to flush ALL time series in a shard.  This time is further divided by groups-per-shard
        flush-interval = 2 minutes

        # TTL for on-disk / C* data.  Data older than this may be purged.
        disk-time-to-live = 3 days

        # amount of time paged chunks should be retained in memory.
        # We need to have a minimum of x hours free blocks or else init won't work.
        demand-paged-chunk-retention-period = 72 hours

        max-chunks-size = 500

        # Fixed amount of memory, in MBs, to allocate for encoded chunks per shard
        shard-memory-mb = 512

        # Max # of partitions or time series the WriteBufferPool can allocate and that can be ingested at a time
        max-num-partitions = 100000

        # Number of subgroups within each shard.  Persistence to a ChunkSink occurs one subgroup at a time, as does
        # recovery from failure.  This many batches of flushes must occur to cover persistence of every partition
        groups-per-shard = 60
      }
    }