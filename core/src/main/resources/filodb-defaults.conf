filodb {

  tasks {
    # Internal task configs for handling lifecycle management and events
    timeouts {
      default = 8000ms
      initialization = 60s
      graceful-stop = 8000ms
      resolve-actor = 10s
    }
  }

  cassandra {
    hosts = ["localhost"]
    port = 9042
    keyspace = "filodb"
    admin-keyspace = "filodb_admin"
    # username = "abc"
    # password = "xyz"
    # read-timeout = 12 s
    # connect-timeout = 5 s
    # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
    # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html

    # Number of parallel chunkset writes at a time
    write-parallelism = 4

    # CQL CREATE KEYSPACE options.  You will want to change these for production.
    keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"

    # NONE, LZ4, SNAPPY.  Compression of CQL traffic over the network.  Turn on for remote clusters,
    # can help by up to 20-30% - unless you use lz4-chunk-compress, in which case leave this off
    cql-compression = "NONE"

    # Compress columnar chunks using LZ4 and store them over CQL that way.
    # Use this instead of cql-compression for the best read performance
    lz4-chunk-compress = false

    # See http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compressSubprop.html for valid values;
    # Use "" to turn off compression.  For the main chunks table only.
    sstable-compression = "LZ4Compressor"

    # Number of stripes the partition keys will be divided into within a shard.
    # The striping/grouping is done so number of rows in a cassandra partition can be controlled.
    partition-list-num-stripes-per-shard = 128

    # retry duration (including jitter) is configured to be little more than chunk-duration / groups-per-shard
    max-retry-attempts = 5
    retry-interval = 10s
    retry-interval-max-jitter = 10s

    ingestion-consistency-level = "ONE"
  }

  spark {
    # The amount of time to wait for dataset creation, truncation, schema changes, etc.
    dataset-ops-timeout = 30s

    # The amount of time to wait for a dataset to finish flushing at the end of a DataFrame write
    flush-timeout = 5m

    # The port used by FiloDB coordinators on executor nodes and the driver to communicate with each
    # other via Akka clustering.  Leaving this commented out will default the port setting to 0, which
    # means find any free open port.
    # executor.port = 5444
    # driver.port = 5555
  }

  # Which MemStore, ChunkSink (for chunk persistence) and MetaStore to use
  # Should be the full class path / FQCN to an implementation of StoreFactory
  store-factory = "filodb.coordinator.TimeSeriesNullStoreFactory"

  columnstore {
    # Number of cache entries for the table cache
    tablecache-size = 50

    # Maximum number of partitions that can be fetched at once that will still fit in
    # one Spark partition/thread when using IN clause in query.
    # If number of partitions are more than this limit then full table scan is performed.
    inquery-partitions-limit = 12
  }

  memstore {
    # Both of these parameters have a lot to do with the shape of the data, and perhaps should be table
    # dataset parameters instread of config parameters.
    # Basically max-chunks-size determines max size of each chunk and affects depth and length of replay
    # upon failures.  chunks-to-keep * max-chunks-size is total data per partition kept in memory.
    chunks-to-keep = 10
    max-chunks-size = 1000

    # Fixed amount of memory, in MBs, to allocate for encoded chunks per shard
    shard-memory-mb = 1536

    # Temporary - max # of partitions the WriteBufferPool can allocate
    max-num-partitions = 100000

    # Number of subgroups within each shard.  Persistence to a ChunkSink occurs one subgroup at a time, as does
    # recovery from failure.  This many batches of flushes must occur to cover persistence of every partition
    groups-per-shard = 60

    # Parallelism of persistance flush tasks. Should never be greater than groups-per-shard
    flush-task-parallelism = 2

  }

  # Set to true to test query result serialization
  test-query-serialization = false

  # for standalone worker cluster configuration, see akka-bootstrapper

  # dataset-definitions:
  # See FiloServer.scala for a way to automatically define datasets at startup

  metrics-logger {
    # Change to true to log internal metrics every tick (10s default, see Kamon.io docs)
    enabled = false

    # Filter different metrics to receive, can be a pattern, empty string to disable
    # ** = log everything / match all
    # trace = **
    # trace-segment = **
    # counter = **
    # gauge = **
    # histogram = **
    # min-max-counter = **
  }

  # Number of threads for reactive ingestion thread pool.
  # Right now ingestion for each shard takes place in its own thread
  ingestion-threads = 8

  hive {
    # Uncomment the below to enable automatic syncing of FiloDB datasets into Hive Metastore external
    # tables so that one does not need to register tables manually with the Hive store.
    # FiloDB tables in the cassandra keyspace below will be synced to the Hive database name below.
    # database-name = "filodb"
  }
}

akka {
  akka.extensions = ["filodb.coordinator.FilodbCluster",
                     "com.romix.akka.serialization.kryo.KryoSerializationExtension$"]

  # Use SLF4J for deployed environment logging
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # will filter the log events using the backend configuration
  # (e.g. logback.xml) before they are published to the event bus.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  loglevel = "INFO"
  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    debug {
      # To enable logging of every Akka message received by the various actors, uncomment the lines below,
      # then change the loglevel above to DEBUG
      # receive = on
      # autoreceive = on
      lifecycle = on
    }

    # For details of kryo section see https://github.com/romix/akka-kryo-serialization
    kryo {
      # TODO: turn this off once finished debugging Kryo classes for serialization
      implicit-registration-logging = "true"

      kryo-custom-serializer-init = "filodb.coordinator.client.KryoInit"

      # Make the buffer size bigger as we send out chunks quite often
      buffer-size = 65536

      # automatic means fall back to FQCN's if class is not pre-registered.  This is safer.
      idstrategy = "automatic"

      mappings {
        # Set standard IDs here from say Scala up to 99.
        # DO NOT INCLUDE FiloDB classes here.  Those are better to put in Serializer.scala so they can be typechecked.
        "scala.Some" = 64
        "scala.Tuple2" = 65
        "scala.None$" = 66
        "scala.collection.immutable.Nil$" = 67
        "scala.collection.immutable.$colon$colon" = 68
        "scala.collection.mutable.ArrayBuffer" = 69
        "scala.collection.immutable.Vector" = 70
      }
    }

    serializers {
      filoingest = "filodb.coordinator.client.IngestRowsSerializer"
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }

    serialization-bindings {
      "filodb.coordinator.client.IngestionCommands$IngestRows" = filoingest
      "filodb.coordinator.client.QueryCommand" = kryo
      "filodb.coordinator.client.QueryResponse" = kryo
      "filodb.core.query.Result" = kryo
      "filodb.core.query.ExecPlan" = kryo
      "filodb.coordinator.client.DatasetCommands$CreateDataset" = kryo
    }

    # Just the defaults to start with. TODO optimize and pick the executor needed.
    shard-status-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Configuration for the fork join pool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 2
        # Parallelism (threads) ... ceil(available processors * factor)
        parallelism-factor = 2.0
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 10
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 100
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      # Leave out the hostname, it will be automatically determined.
      # The Akka port will be overridden by filodb.spark.* settings
      port = 0
      send-buffer-size = 512000b
      receive-buffer-size = 512000b
      maximum-frame-size = 10 MiB
    }
  }

  cluster {
    roles = [worker]

    # If a join request fails it will be retried after this period. Disable join retry by specifying "off".
    retry-unsuccessful-join-after = 10s

    # Auto downing is turned off by default.  See
    # http://doc.akka.io/docs/akka/2.3.16/scala/cluster-usage.html#Automatic_vs__Manual_Downing
    # auto-down-unreachable-after = 120s
    metrics.enabled = off
    failure-detector {
      heartbeat-interval = 5s
      acceptable-heartbeat-pause = 15s
      threshold = 12.0
      expected-response-after = 5s
    }
  }

  # Just the defaults to start with. TODO optimize and pick the executor needed.
  shard-status-dispatcher {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 10
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 100
  }
}

