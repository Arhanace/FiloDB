{
  filodb {
    cassandra {
      hosts = ["localhost"]
      port = 9042
      keyspace = "filodb"
      # username = "abc"
      # password = "xyz"
      # read-timeout = 12 s
      # connect-timeout = 5 s
      # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
      # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html
    }

    # Which column and metaStore to use, ie where to persist columnar chunks.
    # Valid values are "cassandra", "in-memory"
    # Only the filodb-spark library pays attention to this, not the CLI.
    store = "cassandra"

    columnstore {
      # Number of cache entries for the table cache
      tablecache-size = 50

      # Number of ChunkRowMap entries to cache
      segment-cache-size = 1000

      # Maximum number of chunks to flush at one time using one Unlogged Batch.
      chunk-batch-size = 16
    }

    memtable {
      # Uncomment to enable mmap-file based memtable for persistence and easy recovery upon crashes.
      # Defaults to in-memory DB only which is lost upon restarts/crashes, but easier for testing.
      # local-filename = "/tmp/filodb.memtable"

      # Maximum rows per dataset/version before ingestRows throws back a PleaseWait.
      max-rows-per-table = 200000

      # Number of rows in memtable before flushes start being triggered
      flush-trigger-rows = 50000

      # The minimum amount of free memory required to accept new rows into memtable
      min-free-mb = 700

      # Chunk size for FiloMemTable - determines minimum # of rows before serializing to chunks
      filo.chunksize = 1000

      # Maximum delay before new rows in FiloMemTable are flushed to WAL and turned into chunks.
      # The bigger this is, the less WAL disk activity, but also the longer before ingestRows
      # returns acknowledgement.  Think of it as max delay before new ingested rows are guaranteed
      # to be recovered.
      flush.interval = 1 s
    }

    # Thread pool size (min and max) for filodb.core reprojection and I/O tasks.
    # NOTE: This definitely needs to be larger than the segment-batch-size, otherwise
    # all of the segments will be blocked waiting for reads.
    core-futures-pool-size = 128
    core-futures-max-pool-size = 128

    # Thread pool queue length.  When queue is full, tasks get run in calling thread.
    core-futures-queue-length = 1024

    reprojector {
      # Number of times to retry a segment write.  Exponential backoff included.
      retries = 3
      retry-base-timeunit = 5 s

      # Number of segments to flush at a time during reprojection.  Serves to throttle ingestion;
      # higher number uses more memory and causes more write contention to the ColumnStore, but may be
      # slightly more efficient.  If tight on memory, or seeing write timeouts, reduce this number.
      # Ingesting small-segment datasets may require this to be raised.
      segment-batch-size = 32
    }
  }

  akka {
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    loglevel = "INFO"
    actor {
      debug {
        # receive = on
        # autoreceive = on
        lifecycle = on
      }
    }
  }
}

